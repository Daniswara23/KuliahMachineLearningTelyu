{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7pQv_SF5N2iG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BIOFCTmUN30H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PU4sRXovEZCW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segmen ini mengimpor pustaka-pustaka penting yang dibutuhkan untuk proyek machine learning:\n",
        "\n",
        "- torch: Pustaka utama untuk deep learning di Python\n",
        "- pandas: Digunakan untuk manipulasi data\n",
        "- numpy: Untuk operasi numerik\n",
        "- sklearn: Menyediakan alat untuk pembagian data, pra-pemrosesan, dan evaluasi model\n",
        "- matplotlib: Untuk visualisasi data dan grafik"
      ],
      "metadata": {
        "id": "oekz-NUyN3Xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPZ38UyQOcdT",
        "outputId": "41ca9d6f-e7b7-4843-bf6e-88be20e96b0b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing\n",
        "def preprocess_data(data):\n",
        "    # Convert categorical variables to numeric\n",
        "    le = LabelEncoder()\n",
        "    categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'y']\n",
        "\n",
        "    for col in categorical_columns:\n",
        "        data[col] = le.fit_transform(data[col])\n",
        "\n",
        "    # Split features and target\n",
        "    X = data.drop(['y', 'pdays', 'previous', 'poutcome'], axis=1)  # Dropping less relevant columns\n",
        "    y = data['y']\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    return X_scaled, y"
      ],
      "metadata": {
        "id": "90mp8mmzNXJ4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi ini melakukan pra-pemrosesan data dengan beberapa langkah kunci:\n",
        "\n",
        "Mengonversi kolom kategorik menjadi numerik menggunakan LabelEncoder\n",
        "Memisahkan fitur (X) dari variabel target (y)\n",
        "Menghapus beberapa kolom yang dianggap kurang relevan\n",
        "Melakukan penskalaan fitur menggunakan StandardScaler untuk memastikan semua fitur berada dalam skala yang sama"
      ],
      "metadata": {
        "id": "DGoUv7KTN_nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP Model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_layers, neurons_per_layer, activation):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        # Input layer\n",
        "        self.layers.append(nn.Linear(input_dim, neurons_per_layer))\n",
        "\n",
        "        # Hidden layers\n",
        "        for _ in range(hidden_layers - 1):\n",
        "            self.layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))\n",
        "\n",
        "        # Output layer\n",
        "        self.layers.append(nn.Linear(neurons_per_layer, 1))\n",
        "\n",
        "        # Activation function\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        else:  # linear\n",
        "            self.activation = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = self.activation(layer(x))\n",
        "        x = self.layers[-1](x)  # No activation on output layer for regression\n",
        "        return x"
      ],
      "metadata": {
        "id": "Kyj2BRHwNZbh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kelas MLP mendefinisikan arsitektur jaringan saraf:\n",
        "\n",
        "Menerima parameter untuk dimensi input, jumlah lapisan tersembunyi, jumlah neuron, dan fungsi aktivasi\n",
        "Membuat lapisan linear dengan jumlah neuron yang konsisten\n",
        "Mendukung berbagai fungsi aktivasi: ReLU, Sigmoid, Tanh, atau linear\n",
        "Metode forward menentukan bagaimana data mengalir melalui jaringan"
      ],
      "metadata": {
        "id": "6NWlXnI6OCov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, y_train, X_test, y_test, hidden_layers, neurons, activation, epochs, lr, batch_size):\n",
        "    # Convert data to tensors\n",
        "    X_train = torch.FloatTensor(X_train)\n",
        "    y_train = torch.FloatTensor(y_train.values).reshape(-1, 1)\n",
        "    X_test = torch.FloatTensor(X_test)\n",
        "    y_test = torch.FloatTensor(y_test.values).reshape(-1, 1)\n",
        "\n",
        "    # Create data loader\n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = MLP(X_train.shape[1], hidden_layers, neurons, activation)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Training loop\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            train_loss = criterion(model(X_train), y_train).item()\n",
        "            test_loss = criterion(model(X_test), y_test).item()\n",
        "            train_losses.append(train_loss)\n",
        "            test_losses.append(test_loss)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
        "\n",
        "    # Final evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_pred = model(X_train).numpy()\n",
        "        test_pred = model(X_test).numpy()\n",
        "\n",
        "    train_r2 = r2_score(y_train, train_pred)\n",
        "    test_r2 = r2_score(y_test, test_pred)\n",
        "    train_mse = mean_squared_error(y_train, train_pred)\n",
        "    test_mse = mean_squared_error(y_test, test_pred)\n",
        "\n",
        "    return {\n",
        "        'train_r2': train_r2,\n",
        "        'test_r2': test_r2,\n",
        "        'train_mse': train_mse,\n",
        "        'test_mse': test_mse,\n",
        "        'train_losses': train_losses,\n",
        "        'test_losses': test_losses\n",
        "    }"
      ],
      "metadata": {
        "id": "0hJf3xR-NdHS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi ini melakukan pelatihan model dengan fitur-fitur penting:\n",
        "\n",
        "Konversi data ke tensor PyTorch\n",
        "Membuat data loader untuk batch training\n",
        "Inisialisasi model, loss function (MSE), dan optimizer (Adam)\n",
        "Loop pelatihan dengan:\n",
        "\n",
        "Proses training per batch\n",
        "Evaluasi loss di data training dan testing\n",
        "Pencetakan progress setiap 10 epoch\n",
        "\n",
        "\n",
        "Menghitung metrik evaluasi seperti R-squared dan Mean Squared Error"
      ],
      "metadata": {
        "id": "mVYmM1EbOF3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment function\n",
        "def run_experiments(data_path):\n",
        "    # Load and preprocess data\n",
        "    data = pd.read_csv(data_path, sep=';')\n",
        "    X_scaled, y = preprocess_data(data)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Hyperparameter configurations\n",
        "    hidden_layers_list = [1, 2, 3]\n",
        "    neurons_list = [4, 8, 16, 32, 64]\n",
        "    activation_list = ['linear', 'sigmoid', 'relu', 'tanh']\n",
        "    epochs_list = [1, 10, 25, 50, 100, 250]\n",
        "    lr_list = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
        "    batch_size_list = [16, 32, 64, 128, 256, 512]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Base configuration\n",
        "    base_config = {\n",
        "        'hidden_layers': 2,\n",
        "        'neurons': 32,\n",
        "        'activation': 'relu',\n",
        "        'epochs': 50,\n",
        "        'lr': 0.001,\n",
        "        'batch_size': 64\n",
        "    }\n",
        "\n",
        "    # Test each hyperparameter individually\n",
        "    for hidden_layers in hidden_layers_list:\n",
        "        config = base_config.copy()\n",
        "        config['hidden_layers'] = hidden_layers\n",
        "        result = train_model(X_train, y_train, X_test, y_test, **config)\n",
        "        results.append({\n",
        "            'parameter': 'hidden_layers',\n",
        "            'value': hidden_layers,\n",
        "            'metrics': result\n",
        "        })\n",
        "\n",
        "    # Repeat for other hyperparameters...\n",
        "    # (Similar loops for neurons, activation, epochs, lr, batch_size)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "0a3oqIU4NhW2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi ini merancang eksperimen untuk menguji berbagai hyperparameter:\n",
        "\n",
        "Memuat dan membagi data\n",
        "Mendefinisikan rentang hyperparameter untuk pengujian\n",
        "Membuat konfigurasi dasar sebagai titik awal\n",
        "Menjalankan eksperimen dengan mengubah satu hyperparameter pada satu waktu\n",
        "Menyimpan hasil untuk analisis lebih lanjut"
      ],
      "metadata": {
        "id": "10SQyzHDOI0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot results\n",
        "def plot_results(results):\n",
        "    # Implementation of plotting functions to visualize the results\n",
        "    pass"
      ],
      "metadata": {
        "id": "jIziOyctNkAT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_experiments('/content/sample_data/bank-full.csv')\n",
        "    plot_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqy5BEMZNyaN",
        "outputId": "d51567eb-28d2-4931-9267-874faf8a6c3c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/50], Train Loss: 0.0761, Test Loss: 0.0793\n",
            "Epoch [20/50], Train Loss: 0.0742, Test Loss: 0.0775\n",
            "Epoch [30/50], Train Loss: 0.0737, Test Loss: 0.0772\n",
            "Epoch [40/50], Train Loss: 0.0724, Test Loss: 0.0765\n",
            "Epoch [50/50], Train Loss: 0.0719, Test Loss: 0.0761\n",
            "Epoch [10/50], Train Loss: 0.0724, Test Loss: 0.0766\n",
            "Epoch [20/50], Train Loss: 0.0698, Test Loss: 0.0754\n",
            "Epoch [30/50], Train Loss: 0.0682, Test Loss: 0.0757\n",
            "Epoch [40/50], Train Loss: 0.0683, Test Loss: 0.0778\n",
            "Epoch [50/50], Train Loss: 0.0655, Test Loss: 0.0760\n",
            "Epoch [10/50], Train Loss: 0.0707, Test Loss: 0.0759\n",
            "Epoch [20/50], Train Loss: 0.0658, Test Loss: 0.0742\n",
            "Epoch [30/50], Train Loss: 0.0636, Test Loss: 0.0749\n",
            "Epoch [40/50], Train Loss: 0.0617, Test Loss: 0.0767\n",
            "Epoch [50/50], Train Loss: 0.0593, Test Loss: 0.0762\n"
          ]
        }
      ]
    }
  ]
}