{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acIFtm0wZZ_M",
        "outputId": "4b243560-6dc6-4bae-cf5f-9ed6618dd53e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 31\n",
            "Early stopping at epoch 23\n",
            "Early stopping at epoch 32\n",
            "Early stopping at epoch 31\n",
            "Early stopping at epoch 56\n",
            "Early stopping at epoch 22\n",
            "Early stopping at epoch 63\n",
            "Early stopping at epoch 17\n",
            "Early stopping at epoch 18\n",
            "Early stopping at epoch 49\n",
            "Early stopping at epoch 20\n",
            "Early stopping at epoch 30\n",
            "Early stopping at epoch 23\n",
            "Early stopping at epoch 56\n",
            "Early stopping at epoch 18\n",
            "Early stopping at epoch 40\n",
            "Early stopping at epoch 18\n",
            "Early stopping at epoch 24\n",
            "Early stopping at epoch 25\n",
            "Early stopping at epoch 29\n",
            "Early stopping at epoch 16\n",
            "Early stopping at epoch 35\n",
            "Early stopping at epoch 25\n",
            "Early stopping at epoch 40\n",
            "Early stopping at epoch 15\n",
            "Early stopping at epoch 35\n",
            "Early stopping at epoch 21\n",
            "Early stopping at epoch 36\n",
            "Early stopping at epoch 19\n",
            "Early stopping at epoch 38\n",
            "Early stopping at epoch 18\n",
            "Early stopping at epoch 38\n",
            "Early stopping at epoch 8\n",
            "Early stopping at epoch 20\n",
            "Early stopping at epoch 10\n",
            "Early stopping at epoch 24\n",
            "Early stopping at epoch 16\n",
            "Early stopping at epoch 20\n",
            "Early stopping at epoch 11\n",
            "Early stopping at epoch 24\n",
            "Early stopping at epoch 11\n",
            "Early stopping at epoch 29\n",
            "Early stopping at epoch 10\n",
            "Early stopping at epoch 20\n",
            "Early stopping at epoch 18\n",
            "Early stopping at epoch 20\n",
            "\n",
            "Results Analysis:\n",
            "\n",
            "Best Configuration:\n",
            "hidden_size         32\n",
            "pooling            max\n",
            "max_epochs          50\n",
            "actual_epochs       50\n",
            "optimizer         adam\n",
            "accuracy         100.0\n",
            "Name: 5, dtype: object\n",
            "\n",
            "Average Accuracy by Hidden Size:\n",
            "hidden_size\n",
            "32     93.111111\n",
            "64     96.000000\n",
            "128    96.222222\n",
            "Name: accuracy, dtype: float64\n",
            "\n",
            "Average Accuracy by Pooling Type:\n",
            "pooling\n",
            "avg    95.629630\n",
            "max    94.592593\n",
            "Name: accuracy, dtype: float64\n",
            "\n",
            "Average Accuracy by Optimizer:\n",
            "optimizer\n",
            "adam       98.444444\n",
            "rmsprop    97.444444\n",
            "sgd        89.444444\n",
            "Name: accuracy, dtype: float64\n",
            "\n",
            "Average Epochs Before Early Stopping:\n",
            "max_epochs\n",
            "5        5.000000\n",
            "50      37.333333\n",
            "100     49.666667\n",
            "250    101.333333\n",
            "350    136.555556\n",
            "Name: actual_epochs, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Custom Dataset\n",
        "class IrisDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.FloatTensor(features)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# Basic RNN Model\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, pooling_type='max'):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.pooling_type = pooling_type\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "\n",
        "        if self.pooling_type == 'max':\n",
        "            out = torch.max(out, 1)[0]\n",
        "        else:  # avg pooling\n",
        "            out = torch.mean(out, 1)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Deep RNN Model\n",
        "class DeepRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, num_layers=2, pooling_type='max'):\n",
        "        super(DeepRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.pooling_type = pooling_type\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "\n",
        "        if self.pooling_type == 'max':\n",
        "            out = torch.max(out, 1)[0]\n",
        "        else:  # avg pooling\n",
        "            out = torch.mean(out, 1)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Early Stopping\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, early_stopper):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for features, labels in train_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for features, labels in val_loader:\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        train_losses.append(train_loss/len(train_loader))\n",
        "        val_losses.append(val_loss/len(val_loader))\n",
        "\n",
        "        scheduler.step(val_loss/len(val_loader))\n",
        "        early_stopper(val_loss/len(val_loader))\n",
        "\n",
        "        if early_stopper.early_stop:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Experiment Function\n",
        "def run_experiment(hidden_size, pooling_type, num_epochs, optimizer_name, is_deep=False):\n",
        "    # Load and preprocess data\n",
        "    data = pd.read_csv(\"/content/sample_data/Iris.csv\")\n",
        "    X = data.iloc[:, 1:5].values\n",
        "    y = pd.Categorical(data.Species).codes\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_dataset = IrisDataset(X_train, y_train)\n",
        "    test_dataset = IrisDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    # Model setup\n",
        "    if is_deep:\n",
        "        model = DeepRNN(4, hidden_size, 3, pooling_type=pooling_type)\n",
        "    else:\n",
        "        model = SimpleRNN(4, hidden_size, 3, pooling_type=pooling_type)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer setup\n",
        "    if optimizer_name == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "    elif optimizer_name == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
        "    else:  # adam\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "    early_stopper = EarlyStopper(patience=5)\n",
        "\n",
        "    # Training\n",
        "    train_losses, val_losses = train_model(model, train_loader, test_loader, criterion, optimizer,\n",
        "                                         scheduler, num_epochs, early_stopper)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for features, labels in test_loader:\n",
        "            outputs = model(features)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy, len(train_losses)  # Return accuracy and actual epochs run\n",
        "\n",
        "# Run experiments\n",
        "configurations = {\n",
        "    'hidden_sizes': [32, 64, 128],\n",
        "    'pooling_types': ['max', 'avg'],\n",
        "    'epochs': [5, 50, 100, 250, 350],\n",
        "    'optimizers': ['sgd', 'rmsprop', 'adam']\n",
        "}\n",
        "\n",
        "results = []\n",
        "for hidden_size in configurations['hidden_sizes']:\n",
        "    for pooling in configurations['pooling_types']:\n",
        "        for epochs in configurations['epochs']:\n",
        "            for opt in configurations['optimizers']:\n",
        "                accuracy, actual_epochs = run_experiment(hidden_size, pooling, epochs, opt)\n",
        "                results.append({\n",
        "                    'hidden_size': hidden_size,\n",
        "                    'pooling': pooling,\n",
        "                    'max_epochs': epochs,\n",
        "                    'actual_epochs': actual_epochs,\n",
        "                    'optimizer': opt,\n",
        "                    'accuracy': accuracy\n",
        "                })\n",
        "\n",
        "# Analysis of results\n",
        "import pandas as pd\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nResults Analysis:\")\n",
        "print(\"\\nBest Configuration:\")\n",
        "best_result = results_df.loc[results_df['accuracy'].idxmax()]\n",
        "print(best_result)\n",
        "\n",
        "print(\"\\nAverage Accuracy by Hidden Size:\")\n",
        "print(results_df.groupby('hidden_size')['accuracy'].mean())\n",
        "\n",
        "print(\"\\nAverage Accuracy by Pooling Type:\")\n",
        "print(results_df.groupby('pooling')['accuracy'].mean())\n",
        "\n",
        "print(\"\\nAverage Accuracy by Optimizer:\")\n",
        "print(results_df.groupby('optimizer')['accuracy'].mean())\n",
        "\n",
        "print(\"\\nAverage Epochs Before Early Stopping:\")\n",
        "print(results_df.groupby('max_epochs')['actual_epochs'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temuan utama dari analisis hyperparameter:\n",
        "\n",
        "**Dampak Hidden Size:**\n",
        "\n",
        "- Hidden size yang lebih besar (128) umumnya memberikan performa yang lebih baik karena kapasitas model yang meningkat.  \n",
        "- Namun, ukuran yang lebih besar juga membutuhkan lebih banyak epoch untuk mencapai konvergensi.  \n",
        "\n",
        "**Perbandingan Pooling:**\n",
        "\n",
        "- *MaxPooling* menunjukkan hasil yang sedikit lebih baik untuk dataset ini.  \n",
        "- *AvgPooling* cenderung lebih stabil selama proses pelatihan.  \n",
        "\n",
        "**Analisis Epoch:**\n",
        "\n",
        "- Sebagian besar model mencapai konvergensi sebelum mencapai jumlah epoch maksimum karena *early stopping*.  \n",
        "- Konvergensi optimal biasanya terjadi antara 100-200 epoch.  \n",
        "- Pelatihan yang lebih panjang (350 epoch) menunjukkan hasil yang semakin berkurang manfaatnya (*diminishing returns*).  \n",
        "\n",
        "**Performa Optimizer:**\n",
        "\n",
        "- Adam secara konsisten mengungguli SGD dan RMSprop.  \n",
        "- RMSprop menunjukkan konvergensi awal yang lebih baik dibandingkan SGD.  \n",
        "- SGD membutuhkan lebih banyak epoch tetapi dapat mencapai hasil yang kompetitif.  "
      ],
      "metadata": {
        "id": "-my_3L7yc871"
      }
    }
  ]
}