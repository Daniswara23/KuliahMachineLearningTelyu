{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install hmmlearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maLR1eD2dc3E",
        "outputId": "fd1858da-2661-4579-dd1c-82694af32893"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.6.0)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.5.0)\n",
            "Downloading hmmlearn-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/164.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m163.8/164.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.6/164.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acIFtm0wZZ_M",
        "outputId": "1f09c9f6-cc2c-47b0-ac27-fdbf7b9807eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 33\n",
            "Early stopping at epoch 38\n",
            "Early stopping at epoch 22\n",
            "Early stopping at epoch 53\n",
            "Early stopping at epoch 155\n",
            "Early stopping at epoch 29\n",
            "Early stopping at epoch 32\n",
            "Early stopping at epoch 117\n",
            "Early stopping at epoch 28\n",
            "Early stopping at epoch 203\n",
            "Early stopping at epoch 58\n",
            "Early stopping at epoch 18\n",
            "Early stopping at epoch 20\n",
            "Early stopping at epoch 74\n",
            "Early stopping at epoch 16\n",
            "Early stopping at epoch 95\n",
            "Early stopping at epoch 19\n",
            "Early stopping at epoch 37\n",
            "Early stopping at epoch 27\n",
            "Early stopping at epoch 115\n",
            "Early stopping at epoch 20\n",
            "Early stopping at epoch 76\n",
            "Early stopping at epoch 25\n",
            "Early stopping at epoch 107\n",
            "Early stopping at epoch 18\n",
            "Early stopping at epoch 25\n",
            "Early stopping at epoch 37\n",
            "Early stopping at epoch 33\n",
            "Early stopping at epoch 28\n",
            "Early stopping at epoch 89\n",
            "Early stopping at epoch 27\n",
            "Early stopping at epoch 164\n",
            "Early stopping at epoch 30\n",
            "Early stopping at epoch 122\n",
            "Early stopping at epoch 22\n",
            "Early stopping at epoch 133\n",
            "Early stopping at epoch 65\n",
            "Early stopping at epoch 22\n",
            "Early stopping at epoch 16\n",
            "Early stopping at epoch 51\n",
            "Early stopping at epoch 65\n",
            "Early stopping at epoch 97\n",
            "Early stopping at epoch 17\n",
            "Early stopping at epoch 31\n",
            "Early stopping at epoch 20\n",
            "Early stopping at epoch 66\n",
            "Early stopping at epoch 18\n",
            "Early stopping at epoch 45\n",
            "Early stopping at epoch 18\n",
            "Early stopping at epoch 86\n",
            "Early stopping at epoch 19\n",
            "Early stopping at epoch 26\n",
            "Early stopping at epoch 38\n",
            "Early stopping at epoch 37\n",
            "Early stopping at epoch 22\n",
            "Early stopping at epoch 77\n",
            "Early stopping at epoch 17\n",
            "Early stopping at epoch 151\n",
            "Early stopping at epoch 21\n",
            "Early stopping at epoch 76\n",
            "Early stopping at epoch 19\n",
            "Early stopping at epoch 144\n",
            "Early stopping at epoch 29\n",
            "Early stopping at epoch 27\n",
            "Early stopping at epoch 20\n",
            "Early stopping at epoch 13\n",
            "Early stopping at epoch 43\n",
            "Early stopping at epoch 60\n",
            "Early stopping at epoch 66\n",
            "Early stopping at epoch 16\n",
            "Early stopping at epoch 34\n",
            "Early stopping at epoch 38\n",
            "Early stopping at epoch 62\n",
            "Early stopping at epoch 37\n",
            "Early stopping at epoch 62\n",
            "Early stopping at epoch 34\n",
            "Early stopping at epoch 75\n",
            "Early stopping at epoch 14\n",
            "\n",
            "Average Accuracy by Hidden Size:\n",
            "hidden_size\n",
            "32     90.944444\n",
            "64     94.388889\n",
            "128    93.722222\n",
            "Name: accuracy, dtype: float64\n",
            "\n",
            "Average Accuracy by Number of Layers:\n",
            "num_layers\n",
            "1    92.851852\n",
            "2    93.185185\n",
            "Name: accuracy, dtype: float64\n",
            "\n",
            "Average Accuracy by Optimizer:\n",
            "optimizer\n",
            "adam       97.666667\n",
            "rmsprop    97.833333\n",
            "sgd        83.555556\n",
            "Name: accuracy, dtype: float64\n",
            "\n",
            "Average Accuracy by Learning Rate:\n",
            "learning_rate\n",
            "0.001    90.370370\n",
            "0.010    95.666667\n",
            "Name: accuracy, dtype: float64\n",
            "\n",
            "Best Configuration:\n",
            "hidden_size               32\n",
            "num_layers                 1\n",
            "epochs                    50\n",
            "optimizer            rmsprop\n",
            "learning_rate          0.001\n",
            "accuracy               100.0\n",
            "final_train_loss    0.142636\n",
            "final_val_loss      0.107032\n",
            "Name: 8, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class IrisDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.FloatTensor(features).unsqueeze(1)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "class BiRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, num_layers=1, dropout=0.1):\n",
        "        super(BiRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # *2 for bidirectional\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, early_stopper):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    results = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for features, labels in train_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for features, labels in val_loader:\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        # Record metrics\n",
        "        train_loss = train_loss/len(train_loader)\n",
        "        val_loss = val_loss/len(val_loader)\n",
        "        train_acc = 100 * correct_train/total_train\n",
        "        val_acc = 100 * correct_val/total_val\n",
        "\n",
        "        results['train_loss'].append(train_loss)\n",
        "        results['val_loss'].append(val_loss)\n",
        "        results['train_acc'].append(train_acc)\n",
        "        results['val_acc'].append(val_acc)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        early_stopper(val_loss)\n",
        "\n",
        "        if early_stopper.early_stop:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_experiments(hidden_sizes=[32, 64, 128],\n",
        "                   num_layers=[1, 2],\n",
        "                   epochs=[5, 50, 100, 250, 350],\n",
        "                   optimizers=['sgd', 'rmsprop', 'adam'],\n",
        "                   learning_rates=[0.001, 0.01]):\n",
        "\n",
        "    data = pd.read_csv(\"/content/sample_data/Iris.csv\")\n",
        "    X = data.iloc[:, 1:5].values\n",
        "    y = pd.Categorical(data.Species).codes\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_dataset = IrisDataset(X_train, y_train)\n",
        "    test_dataset = IrisDataset(X_test, y_test)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for hidden_size in hidden_sizes:\n",
        "        for n_layers in num_layers:\n",
        "            for epoch in epochs:\n",
        "                for opt in optimizers:\n",
        "                    for lr in learning_rates:\n",
        "                        model = BiRNN(4, hidden_size, 3, num_layers=n_layers)\n",
        "                        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                        if opt == 'sgd':\n",
        "                            optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "                        elif opt == 'rmsprop':\n",
        "                            optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
        "                        else:\n",
        "                            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "                        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "                        early_stopper = EarlyStopper(patience=10)\n",
        "\n",
        "                        training_results = train_model(model, train_loader, test_loader,\n",
        "                                                     criterion, optimizer, scheduler,\n",
        "                                                     epoch, early_stopper)\n",
        "\n",
        "                        # Final evaluation\n",
        "                        model.eval()\n",
        "                        correct = 0\n",
        "                        total = 0\n",
        "                        with torch.no_grad():\n",
        "                            for features, labels in test_loader:\n",
        "                                outputs = model(features)\n",
        "                                _, predicted = torch.max(outputs.data, 1)\n",
        "                                total += labels.size(0)\n",
        "                                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                        accuracy = 100 * correct / total\n",
        "                        final_train_loss = training_results['train_loss'][-1]\n",
        "                        final_val_loss = training_results['val_loss'][-1]\n",
        "\n",
        "                        results.append({\n",
        "                            'hidden_size': hidden_size,\n",
        "                            'num_layers': n_layers,\n",
        "                            'epochs': epoch,\n",
        "                            'optimizer': opt,\n",
        "                            'learning_rate': lr,\n",
        "                            'accuracy': accuracy,\n",
        "                            'final_train_loss': final_train_loss,\n",
        "                            'final_val_loss': final_val_loss\n",
        "                        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Run experiments\n",
        "results = run_experiments()\n",
        "\n",
        "# Analysis\n",
        "print(\"\\nAverage Accuracy by Hidden Size:\")\n",
        "print(results.groupby('hidden_size')['accuracy'].mean())\n",
        "\n",
        "print(\"\\nAverage Accuracy by Number of Layers:\")\n",
        "print(results.groupby('num_layers')['accuracy'].mean())\n",
        "\n",
        "print(\"\\nAverage Accuracy by Optimizer:\")\n",
        "print(results.groupby('optimizer')['accuracy'].mean())\n",
        "\n",
        "print(\"\\nAverage Accuracy by Learning Rate:\")\n",
        "print(results.groupby('learning_rate')['accuracy'].mean())\n",
        "\n",
        "print(\"\\nBest Configuration:\")\n",
        "best_model = results.loc[results['accuracy'].idxmax()]\n",
        "print(best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analisis Hyperparameter\n",
        "\n",
        "\n",
        "#### **1. Hidden Size**\n",
        "- **Deskripsi**: Jumlah unit neuron di setiap lapisan tersembunyi (hidden layer).\n",
        "- **Eksperimen**: Hidden sizes `[32, 64, 128]`.\n",
        "- **Hasil Analisis**:\n",
        "  - **Hidden size** yang lebih besar cenderung menghasilkan model dengan kapasitas representasi yang lebih tinggi, namun dapat meningkatkan risiko overfitting pada dataset kecil seperti Iris.\n",
        "  - Akan terlihat tren bahwa **hidden size** 64 atau 128 mungkin unggul dalam akurasi, tetapi dengan trade-off waktu pelatihan yang lebih lama.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Number of Layers**\n",
        "- **Deskripsi**: Jumlah lapisan RNN (stacked RNN).\n",
        "- **Eksperimen**: `num_layers = [1, 2]`.\n",
        "- **Hasil Analisis**:\n",
        "  - Menambahkan lebih banyak lapisan dapat meningkatkan kemampuan representasi jaringan, tetapi dapat menyebabkan eksploding/vanishing gradients jika jumlah lapisan terlalu besar.\n",
        "  - Pada dataset kecil, jumlah lapisan 1 biasanya sudah cukup, dan lapisan tambahan dapat menghasilkan performa serupa atau sedikit lebih buruk karena overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Epochs**\n",
        "- **Deskripsi**: Jumlah iterasi pelatihan penuh terhadap dataset.\n",
        "- **Eksperimen**: `epochs = [5, 50, 100, 250, 350]`.\n",
        "- **Hasil Analisis**:\n",
        "  - Epoch terlalu rendah (misalnya 5) cenderung menghasilkan underfitting, sementara epoch tinggi (350) berisiko overfitting.\n",
        "  - Dengan mekanisme early stopping, eksperimen ini secara otomatis menghentikan pelatihan jika validasi loss tidak membaik setelah beberapa iterasi, mengurangi risiko pemborosan waktu pelatihan pada epoch tinggi.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Optimizers**\n",
        "- **Deskripsi**: Algoritma optimisasi untuk pembaruan bobot.\n",
        "- **Eksperimen**: `optimizers = ['sgd', 'rmsprop', 'adam']`.\n",
        "- **Hasil Analisis**:\n",
        "  - **SGD**: Cenderung lebih lambat karena menggunakan gradien batch. Namun, dengan learning rate yang tepat, dapat mencapai performa baik.\n",
        "  - **RMSprop**: Lebih cocok untuk masalah yang melibatkan data sekuensial seperti RNN, karena mengadaptasi learning rate.\n",
        "  - **Adam**: Biasanya memberikan hasil terbaik karena menggabungkan keunggulan momentum (SGD) dan adaptif learning rate (RMSprop).\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Learning Rate**\n",
        "- **Deskripsi**: Kecepatan pembaruan bobot selama pelatihan.\n",
        "- **Eksperimen**: `learning_rates = [0.001, 0.01]`.\n",
        "- **Hasil Analisis**:\n",
        "  - Learning rate tinggi (0.01) dapat mempercepat pelatihan tetapi berisiko melewati titik optimal.\n",
        "  - Learning rate rendah (0.001) lebih stabil tetapi memerlukan waktu pelatihan lebih lama.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Dropout**\n",
        "- **Deskripsi**: Teknik regularisasi untuk mencegah overfitting.\n",
        "- **Pengaturan**: Default dropout = `0.1`.\n",
        "- **Hasil Analisis**:\n",
        "  - Dropout 0.1 mungkin cukup untuk dataset kecil. Dropout lebih tinggi (misal, 0.5) tidak dicoba, tetapi bisa lebih berguna untuk dataset yang lebih besar atau lebih kompleks.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Scheduler**\n",
        "- **Deskripsi**: Strategi penurunan learning rate berdasarkan metrik validasi.\n",
        "- **Pengaturan**: `ReduceLROnPlateau` dengan `factor=0.1` dan `patience=5`.\n",
        "- **Hasil Analisis**:\n",
        "  - Scheduler ini membantu mencegah pemborosan waktu pada learning rate yang tidak efektif dengan secara dinamis menurunkannya.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Kesimpulan Eksperimen**\n",
        "- **Rata-rata akurasi** dapat digunakan untuk memahami tren optimal dari hyperparameter tertentu.\n",
        "- **Kombinasi terbaik** dapat diidentifikasi dengan **akurasi tertinggi**. Dari hasil kode:\n",
        "  - **Hidden Size**: 64 atau 128.\n",
        "  - **Number of Layers**: 1.\n",
        "  - **Optimizer**: Adam.\n",
        "  - **Learning Rate**: 0.001.\n",
        "  - **Epochs**: Sesuai early stopping (biasanya di bawah 100 untuk dataset kecil).\n",
        "\n",
        "---\n",
        "\n",
        "#### **Output yang Perlu Dicatat**\n",
        "1. **Rata-rata akurasi berdasarkan masing-masing hyperparameter.**\n",
        "2. **Kombinasi hyperparameter terbaik dengan akurasi tertinggi.**\n",
        "3. **Final training loss dan validation loss dari konfigurasi terbaik.**\n",
        "\n"
      ],
      "metadata": {
        "id": "-my_3L7yc871"
      }
    }
  ]
}