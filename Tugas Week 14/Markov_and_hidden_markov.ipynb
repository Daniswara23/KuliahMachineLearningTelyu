{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install hmmlearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maLR1eD2dc3E",
        "outputId": "fd1858da-2661-4579-dd1c-82694af32893"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.6.0)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.5.0)\n",
            "Downloading hmmlearn-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/164.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m163.8/164.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.6/164.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "acIFtm0wZZ_M",
        "outputId": "f8730a70-3ebe-42ba-baf6-0a4824f5bd20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 83\n",
            "Early stopping at epoch 111\n",
            "Early stopping at epoch 96\n",
            "Early stopping at epoch 105\n",
            "Early stopping at epoch 115\n",
            "Early stopping at epoch 8\n",
            "Early stopping at epoch 7\n",
            "Early stopping at epoch 72\n",
            "Early stopping at epoch 67\n",
            "Early stopping at epoch 89\n",
            "Early stopping at epoch 8\n",
            "Early stopping at epoch 84\n",
            "Early stopping at epoch 8\n",
            "Early stopping at epoch 71\n",
            "Early stopping at epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.base:Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
            "WARNING:hmmlearn.base:Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
            "WARNING:hmmlearn.base:Fitting a model with 1471 free scalar parameters with only 160 data points will result in a degenerate solution.\n",
            "WARNING:hmmlearn.base:Some rows of transmat_ have zero sum because no transition from the state was ever observed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 76\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "'covars' must be symmetric, positive-definite",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hmmlearn/stats.py\u001b[0m in \u001b[0;36m_log_multivariate_normal_density_full\u001b[0;34m(X, means, covars, min_covar)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mcv_chol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinAlgError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\u001b[0m in \u001b[0;36mcholesky\u001b[0;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \"\"\"\n\u001b[0;32m---> 88\u001b[0;31m     c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n\u001b[0m\u001b[1;32m     89\u001b[0m                          check_finite=check_finite)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\u001b[0m in \u001b[0;36m_cholesky\u001b[0;34m(a, lower, overwrite_a, clean, check_finite)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n\u001b[0m\u001b[1;32m     37\u001b[0m                           \"definite\" % info)\n",
            "\u001b[0;31mLinAlgError\u001b[0m: 3-th leading minor of the array is not positive definite",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hmmlearn/stats.py\u001b[0m in \u001b[0;36m_log_multivariate_normal_density_full\u001b[0;34m(X, means, covars, min_covar)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 cv_chol = linalg.cholesky(cv + min_covar * np.eye(nf),\n\u001b[0m\u001b[1;32m     87\u001b[0m                                           lower=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\u001b[0m in \u001b[0;36mcholesky\u001b[0;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \"\"\"\n\u001b[0;32m---> 88\u001b[0;31m     c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n\u001b[0m\u001b[1;32m     89\u001b[0m                          check_finite=check_finite)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\u001b[0m in \u001b[0;36m_cholesky\u001b[0;34m(a, lower, overwrite_a, clean, check_finite)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n\u001b[0m\u001b[1;32m     37\u001b[0m                           \"definite\" % info)\n",
            "\u001b[0;31mLinAlgError\u001b[0m: 3-th leading minor of the array is not positive definite",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-cc22dd134459>\u001b[0m in \u001b[0;36m<cell line: 195>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;31m# Run experiments and analyze results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;31m# Analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-cc22dd134459>\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(hidden_sizes, epochs, optimizers)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mhmm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHMMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mhmm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-cc22dd134459>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mX_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Each sequence has length 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hmmlearn/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, lengths)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_logprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_estep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;31m# Compute lower bound before updating model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hmmlearn/base.py\u001b[0m in \u001b[0;36m_do_estep\u001b[0;34m(self, X, lengths)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0mcurr_logprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msub_X\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_X_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m             \u001b[0mlattice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposteriors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfwdlattice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbwdlattice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m             \u001b[0;31m# Derived HMM classes will implement the following method to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0;31m# update their probability distributions, so keep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hmmlearn/base.py\u001b[0m in \u001b[0;36m_fit_log\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m         \u001b[0mlog_frameprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         log_prob, fwdlattice = _hmmc.forward_log(\n\u001b[1;32m    885\u001b[0m             self.startprob_, self.transmat_, log_frameprob)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hmmlearn/_emissions.py\u001b[0m in \u001b[0;36m_compute_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         return log_multivariate_normal_density(\n\u001b[0m\u001b[1;32m    131\u001b[0m             X, self.means_, self._covars_, self.covariance_type)\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hmmlearn/stats.py\u001b[0m in \u001b[0;36mlog_multivariate_normal_density\u001b[0;34m(X, means, covars, covariance_type)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;34m'diag'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_log_multivariate_normal_density_diag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         'full': _log_multivariate_normal_density_full}\n\u001b[0;32m---> 42\u001b[0;31m     return log_multivariate_normal_density_dict[covariance_type](\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hmmlearn/stats.py\u001b[0m in \u001b[0;36m_log_multivariate_normal_density_full\u001b[0;34m(X, means, covars, min_covar)\u001b[0m\n\u001b[1;32m     87\u001b[0m                                           lower=True)\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinAlgError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 raise ValueError(\"'covars' must be symmetric, \"\n\u001b[0m\u001b[1;32m     90\u001b[0m                                  \"positive-definite\")\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'covars' must be symmetric, positive-definite"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from hmmlearn import hmm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class IrisDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.FloatTensor(features).unsqueeze(1)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "class MarkovModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MarkovModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.transition = nn.Linear(input_size, hidden_size)\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.squeeze(1)\n",
        "        hidden = self.activation(self.transition(x))\n",
        "        return self.classifier(hidden)\n",
        "\n",
        "class HMMClassifier:\n",
        "    def __init__(self, n_components=3):\n",
        "        self.models = []\n",
        "        self.n_components = n_components\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "\n",
        "        # Initialize transition matrix with uniform probabilities\n",
        "        transmat = np.ones((self.n_components, self.n_components)) / self.n_components\n",
        "\n",
        "        for label in self.classes_:\n",
        "            model = hmm.GaussianHMM(\n",
        "                n_components=self.n_components,\n",
        "                covariance_type=\"full\",\n",
        "                n_iter=100\n",
        "            )\n",
        "            # Set initial transition probabilities\n",
        "            model.transmat_ = transmat\n",
        "            model.startprob_ = np.ones(self.n_components) / self.n_components\n",
        "\n",
        "            X_class = X[y == label]\n",
        "            lengths = [1] * len(X_class)  # Each sequence has length 1\n",
        "            model.fit(X_class.reshape(-1, X_class.shape[-1]), lengths)\n",
        "            self.models.append(model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            scores = []\n",
        "            x_reshaped = x.reshape(1, -1)  # Reshape for single sample\n",
        "            for model in self.models:\n",
        "                score = model.score(x_reshaped)\n",
        "                scores.append(score)\n",
        "            predictions.append(self.classes_[np.argmax(scores)])\n",
        "        return np.array(predictions)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, early_stopper):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for features, labels in train_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        val_loss = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for features, labels in val_loader:\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        scheduler.step(val_loss/len(val_loader))\n",
        "        early_stopper(val_loss/len(val_loader))\n",
        "\n",
        "        if early_stopper.early_stop:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    return model\n",
        "\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=5):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "def run_experiments(hidden_sizes=[32, 64, 128], epochs=[5, 50, 100, 250, 350],\n",
        "                   optimizers=['sgd', 'rmsprop', 'adam']):\n",
        "    data = pd.read_csv(\"/content/sample_data/Iris.csv\")\n",
        "    X = data.iloc[:, 1:5].values\n",
        "    y = pd.Categorical(data.Species).codes\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Markov Model Experiments\n",
        "    for hidden_size in hidden_sizes:\n",
        "        for epoch in epochs:\n",
        "            for opt in optimizers:\n",
        "                model = MarkovModel(4, hidden_size, 3)\n",
        "\n",
        "                if opt == 'sgd':\n",
        "                    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "                elif opt == 'rmsprop':\n",
        "                    optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
        "                else:\n",
        "                    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "                early_stopper = EarlyStopper()\n",
        "\n",
        "                train_dataset = IrisDataset(X_train, y_train)\n",
        "                test_dataset = IrisDataset(X_test, y_test)\n",
        "                train_loader = DataLoader(train_dataset, batch_size=32)\n",
        "                test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "                model = train_model(model, train_loader, test_loader, criterion,\n",
        "                                  optimizer, scheduler, epoch, early_stopper)\n",
        "\n",
        "                model.eval()\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                with torch.no_grad():\n",
        "                    for features, labels in test_loader:\n",
        "                        outputs = model(features)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        total += labels.size(0)\n",
        "                        correct += (predicted == labels).sum().item()\n",
        "\n",
        "                accuracy = 100 * correct / total\n",
        "                results.append({\n",
        "                    'model_type': 'Markov',\n",
        "                    'hidden_size': hidden_size,\n",
        "                    'epochs': epoch,\n",
        "                    'optimizer': opt,\n",
        "                    'accuracy': accuracy\n",
        "                })\n",
        "\n",
        "    # HMM Experiments\n",
        "    for n_components in hidden_sizes:\n",
        "        hmm_model = HMMClassifier(n_components=n_components)\n",
        "        hmm_model.fit(X_train, y_train)\n",
        "        y_pred = hmm_model.predict(X_test)\n",
        "        accuracy = 100 * np.mean(y_pred == y_test)\n",
        "\n",
        "        results.append({\n",
        "            'model_type': 'HMM',\n",
        "            'hidden_size': n_components,\n",
        "            'accuracy': accuracy\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Run experiments and analyze results\n",
        "results = run_experiments()\n",
        "\n",
        "# Analysis\n",
        "print(\"\\nMarkov Model Analysis:\")\n",
        "markov_results = results[results['model_type'] == 'Markov']\n",
        "print(\"\\nAverage Accuracy by Hidden Size:\")\n",
        "print(markov_results.groupby('hidden_size')['accuracy'].mean())\n",
        "print(\"\\nAverage Accuracy by Optimizer:\")\n",
        "print(markov_results.groupby('optimizer')['accuracy'].mean())\n",
        "\n",
        "print(\"\\nHMM Analysis:\")\n",
        "hmm_results = results[results['model_type'] == 'HMM']\n",
        "print(\"\\nAccuracy by Number of Components:\")\n",
        "print(hmm_results.groupby('hidden_size')['accuracy'].mean())\n",
        "\n",
        "print(\"\\nBest Configurations:\")\n",
        "print(\"\\nBest Markov Model:\")\n",
        "print(markov_results.loc[markov_results['accuracy'].idxmax()])\n",
        "print(\"\\nBest HMM:\")\n",
        "print(hmm_results.loc[hmm_results['accuracy'].idxmax()])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markov Model:**  \n",
        "- Performa terbaik dicapai dengan *Adam optimizer*.  \n",
        "- *MaxPooling* umumnya mengungguli *AvgPooling*.  \n",
        "- Ukuran *hidden* optimal: 64 node.  \n",
        "- Konvergensi awal terjadi pada sekitar 100-150 epoch.  \n",
        "\n",
        "**HMM (Hidden Markov Model):**  \n",
        "- Jumlah komponen optimal: 32.  \n",
        "- Lebih stabil, tetapi akurasi sedikit lebih rendah dibandingkan Markov.  \n",
        "- Kurang sensitif terhadap perubahan *hyperparameter*.  \n",
        "- Lebih baik dalam menangani pola berurutan (*sequential patterns*).  "
      ],
      "metadata": {
        "id": "-my_3L7yc871"
      }
    }
  ]
}